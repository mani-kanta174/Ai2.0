	‚Ä¢ Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. 
	‚Ä¢ Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. 
	‚Ä¢ Used leverage domain-specific or organizational knowledge bases, without requiring model retraining.

Raw Documents / Knowledge Sources
Sources:
	‚Ä¢ PDFs
	‚Ä¢ Confluence/Wiki pages
	‚Ä¢ Jira tickets
	‚Ä¢ API documentation
	‚Ä¢ Internal policies
	‚Ä¢ Excel sheets
Goal ‚Üí Bring all knowledge into the system.

Text Splitter / Chunking Layer
	1. Converts the entire document into clean plain text
	2. Breaks the document into small, meaningful chunks
	3. Creates overlap between chunks
	4. Adds metadata to every chunk
	5. Prepares final chunk objects for embedding

	1. Converts the entire document into clean plain text
		a. Extracts text from PDFs, Word, Jira, Wiki, Websites, etc.
		b. Removes noise like headers, footers, page numbers.
		c. Normalizes the text (spaces, newlines, encoding fixes).
		d. ‚Üí Purpose: Make the raw document usable for embeddings.
	
	2. Breaks the document into small, meaningful chunks
		a. Splits large documents into 300‚Äì500 token blocks.
		b. Uses rules like paragraphs, headings, or semantic boundaries.
			i. Break at a new paragraph (empty line, \n\n).
			ii. Break when you see a heading.
			iii. Break when the meaning shifts even if there is no heading.
		c. Why do we split?
			i. To avoid sending large documents directly to embeddings
			ii. To store meaningful units in vector DB
		d. ‚Üí Purpose: LLMs and vector DB work best with small units of meaning.
	
	3. Creates overlap between chunks
		a. Adds 20‚Äì30% overlap to avoid losing context.
		b. Example:
			i. Chunk 1 = sentences 1‚Äì8
			ii. Chunk 2 = sentences 6‚Äì14
		c. ‚Üí Purpose: Prevents missing important details at chunk boundaries.
	4. Adds metadata to every chunk
		a. Each chunk gets identifiers like:
			i. document name
			ii. section or heading
			iii. page number
			iv. source type (PDF/Jira/Webpage)
			v. timestamp
			vi. ‚Üí Purpose: Improves filtering and accurate retrieval during search.
	5. Prepares final chunk objects for embedding
		a. After splitting, each chunk looks like:
		
		b. {
  "id": "doc12_chunk04",
  "text": "How to reset VPN password...",
  "metadata": { "source": "Wiki", "page": 2 }
}
		c. ‚Üí Purpose: These chunks are now ready to be transformed into vectors.

Embedding Generator
(LLM Embedding Model ‚Üí Vector Representation)
	‚Ä¢ The Embedding Generator is the step in which we pass each chunk into an Embedding Model to convert text into semantic vectors.
	‚Ä¢ The Embedding Generator takes each chunk of text and converts it into a numerical vector(mathematical representations)‚Äîa list of floating-point numbers.
	‚Ä¢ These numbers represent the semantic meaning of the text.
	‚Ä¢ These vectors (embeddings) allow the system to search by meaning, not by keywords.
	‚Ä¢  Traditional search (keyword search) fails when:
		‚Ä¢ User does not use exact document words
	But embeddings solve this:
		‚Ä¢ They convert text ‚Üí meaning
		‚Ä¢ The system retrieves documents that mean the same, even if words differ
	‚Ä¢ Text embeddings
		‚Ä¢ For normal text
	Code embeddings
		‚Ä¢ Useful for GitHub/Jira queries
	Multi-modal embeddings
		‚Ä¢ Images + text (CLIP etc.)
	
Vector Database (Storage Layer)
(Pinecone, Chroma, Weaviate, Milvus)
	‚Ä¢ Once chunks are converted into embeddings (vectors), the vector database stores them in a way that allows extremely fast semantic similarity search(SSS).
	‚Ä¢ A vector database is not like MongoDB or SQL.
	‚Ä¢ Instead, it is specially designed to store high-dimensional vectors and retrieve the closest ones based on meaning.
	‚Ä¢ Stores vectors + metadata
	For each chunk, the DB stores:
		‚Ä¢ the vector (embeddings)
		‚Ä¢ chunk text
		‚Ä¢ source document
		‚Ä¢ page number
		‚Ä¢ tags / metadata (like Jira ticket, wiki page, version)
	Example entry:
	
	id: "policy_chunk_04"
vector: [0.221, -0.880, 0.112, ...]
metadata: {
  source: "Leave_Policy.pdf",
  page: 2,
  section: "Medical Leave",
  timestamp: 2024
}
	
	Indexing for ultra-fast search
		‚Ä¢ Raw text ‚Üí embeddings (by an embedding model)
		‚Ä¢ Vector DB stores these vectors
		‚Ä¢ HNSW happens after vector generation, not on raw text.
		‚Ä¢ After vectors are stored ‚Üí Vector DB builds the HNSW index
		‚Ä¢ Only after vectors exist, HNSW can link similar vectors
		‚Ä¢ Indexing is based on distance between vector embeddings
		‚Ä¢ vector database uses HNSW to index and store them for fast semantic similarity search(SSS).
		‚Ä¢ Vector DB uses specialized indexing algorithms like:
		‚Ä¢ HNSW (Hierarchical Navigable Small World)
		‚Ä¢ IVF-Faiss
		‚Ä¢ Annoy
	These algorithms allow:
		‚Ä¢ millisecond semantic search
		‚Ä¢ even with millions of vectors
	Interview tip:
	üëâ ‚ÄúHNSW is the most commonly used index for fast approximate nearest neighbor search.‚Äù
	
	Performs Similarity Search (Cosine / Dot Product)
	‚Ä¢ We don‚Äôt manually compute cosine similarity or dot product. The vector database performs all similarity calculations internally. My job is only to generate embeddings and pass them to the vector DB. The DB handles indexing and fast retrieval using HNSW or IVF. This makes similarity search extremely fast and scalable.
	‚Ä¢ results = vector_db.query(vector=query_embedding, top_k=5)
	When a user asks a question:
		‚Ä¢ Convert the query to a vector
		‚Ä¢ Compare it with stored vectors
		‚Ä¢ Find the Top-K nearest chunks
		‚Ä¢ Based on cosine similarity / dot product
		‚Ä¢ Vector DB automatically Chooses cosine or dot product
	Example:
	User Query: ‚ÄúHow do I apply for sick leave?‚Äù
	Vector DB returns chunks like:
		‚Ä¢ ‚ÄúEmployees can take medical leave‚Ä¶‚Äù
		‚Ä¢ ‚ÄúProcedure for applying leave‚Ä¶‚Äù
	NOT:
		‚Ä¢ ‚ÄúPassword reset steps‚Äù
		‚Ä¢ ‚ÄúNetwork configuration‚Äù
	
	Uses Metadata Filters (Very important)
	You can filter results by:
		‚Ä¢ project name
		‚Ä¢ year
		‚Ä¢ department
		‚Ä¢ document type
		‚Ä¢ Jira issue type
	Example:
	
	Give me only Chunks:
Where project = "Project-X"
And category = "Security"
Limit 5
	This helps avoid irrelevant results.
	
	Sends Relevant Chunks to LLM
	After similarity search, the DB returns the most relevant chunks.
	These are added to the prompt context.
	Then the LLM uses this context to generate the final answer.
	This is how RAG prevents hallucination.
	EX -
	SYSTEM:
	You must answer using ONLY the information provided in context.  
	Do not guess.
	CONTEXT:
	Chunk 1:
	Employees are allowed 12 days of sick leave per calendar year.
	Chunk 2:
	Sick leave can be applied through the HR portal under Medical Leave.
	USER QUESTION:
	How many sick leaves do employees get?
	
User Query Input Layer
(User asks a question)

Query Embedding Generator
(Convert query into embedding vector)

def get_query_embedding(query):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )

Vector Database Retrieval Layer
(Similarity Search + Metadata Filtering)
This layer is responsible for finding the most relevant chunks from the vector database using:
	1. Similarity Search ‚Üí Uses cosine similarity / dot product
		‚Ä¢ Cosine similarity: Measures the angle between vectors (how similar directions are).
		‚Ä¢ Dot product: Measures the similarity based on vector magnitude and direction.
	2. Metadata Filtering ‚Üí Filters using fields like:
		‚óã document_id
		‚óã created_date
		‚óã tags
		‚óã author
		‚óã page number
		‚óã category

Context Builder
(Top-K Relevant Chunks Merged)
After the Vector Database Retrieval Layer fetches the most relevant chunks, the Context Builder:
	1. Selects the Top-K chunks
		‚óã "K" is a number you define (e.g., top 5 chunks).
		‚óã These are the chunks that are most relevant to the user query based on similarity search and metadata filtering.
	2. Merges the chunks into a single context
		‚óã The goal is to create a coherent input for the LLM.
		‚óã This avoids overwhelming the model with too many separate chunks and ensures relevant information is consolidated.
Example:
Suppose the retrieved chunks for query "Tourist places in Kerala" are:
	‚Ä¢ Chunk 1: "Kerala is known for its backwaters and houseboats."
	‚Ä¢ Chunk 2: "Varkala beach is a popular tourist spot in Kerala."
	‚Ä¢ Chunk 3: "Munnar is famous for tea plantations and hill stations."
If Top-2 (K=2) is chosen:
	‚Ä¢ Selected chunks: Chunk 1 and Chunk 2
	‚Ä¢ Merged context: "Kerala is known for its backwaters and houseboats. Varkala beach is a popular tourist spot in Kerala."
This merged context is then sent to the LLM to generate an answer to the user query.

LLM Reasoning Layer
(LLM generates grounded answer using context)

Final This layer is the part of a Retrieval-Augmented Generation (RAG) or AI system where the Large Language Model (LLM) processes the retrieved context and generates a final, grounded answer. Essentially, it‚Äôs where the ‚Äúthinking‚Äù happens.
Inputs to this layer:
	1. User Query ‚Äì The question or prompt provided by the user.
	2. Relevant Chunks / Context ‚Äì Retrieved from the vector database after similarity search and metadata filtering. These are usually text segments from documents, FAQs, wikis, or other knowledge sources.
Process:
	1. Context Integration ‚Äì The LLM takes the retrieved chunks and integrates them into its input prompt. This gives it the grounding to answer factually.
	2. Reasoning ‚Äì The LLM processes both the user query and the provided context, performing:
		‚óã Comprehension: Understanding the question.
		‚óã Synthesis: Combining multiple chunks of information to form a coherent answer.
		‚óã Inference / Reasoning: Drawing conclusions, filling gaps, or explaining relationships.
	3. Answer Generation ‚Äì The model outputs a response that is:
		‚óã Grounded in the context (citing sources or directly reflecting retrieved data)
		‚óã Coherent and natural language
Output Layer
(Display the final answer to the user)
<img width="1030" height="7088" alt="image" src="https://github.com/user-attachments/assets/bec00fd0-a779-40aa-9361-9c3cf9f09899" />
